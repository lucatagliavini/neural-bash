ACTIVATION=relu
-0.805157 -0.457350 -1.205274
2.323611 2.323667 -2.323345
0.403922 -0.511471 -1.472848
-0.027325 -0.084523 1.066146
-0.932314 0.095392 -1.643938
-2.264545 -2.264598 2.264513
0.176080 0.362074 1.997362
-0.249315 0.119944 -0.491483
