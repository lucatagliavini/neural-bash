ACTIVATION=relu
-1.234934 0.912410 -0.363221 -0.481786 -0.403899 -1.032594 -0.150362 -0.251458 0.374205
0.686412 -1.154818 0.817539 0.179101 -0.861442 0.277700 -0.234866 -0.376457 0.456243
0.122792 -0.963735 -0.539881 -0.094066 -0.377944 0.163836 0.296834 -0.120219 0.123728
-0.272549 -1.056361 0.548052 -0.578396 -0.226975 -0.820490 -0.464732 -0.047685 0.509227
-1.037494 -0.169257 0.797278 -0.430163 -0.174206 0.323650 -0.468174 -0.354538 -0.056121
0.359136 -0.495625 0.367946 0.188934 0.078434 0.586956 0.769942 0.788229 -0.540996
0.483876 0.055698 -0.387522 -0.392077 -0.400084 -0.215407 0.225781 -0.151082 -0.576458
-0.216669 -0.072616 -0.203596 0.529380 0.207215 0.473267 -0.589449 -0.096673 -0.330881
