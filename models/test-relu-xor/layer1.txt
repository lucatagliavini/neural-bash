ACTIVATION=relu
0.267386 -1.462574 0.479980
-1.927827 0.755041 -0.086419
-0.082144 -0.461528 -1.429902
0.183937 -0.490151 -0.302796
0.879280 -0.743573 -0.226494
0.375065 -1.616027 0.274656
0.610599 -0.965386 -0.183910
0.632318 -1.057921 0.042229
