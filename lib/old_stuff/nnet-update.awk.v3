# awk: lib/functions/nnet-update.awk
#
# Author: Luca Tagliavini
#
# Parametri:
# awk \
#  -v input_file="tmp/xor-test/input_only.txt" \   	# Per gli altri layer: -v input_file="tmp/xor-test/layer1_with_bias.tmp" \
#  -v grad_file="tmp/xor-test/layer1-grad.txt" \
#  -v weights_file="models/xor/layer1.txt" \
#  -v output_file="tmp/xor-test/layer1-updated.txt" \
#  -v learning_rate=0.1 \
#  -v debug=1 \
#  -f lib/functions/math.awk \
#  -f lib/functions/nnet-update.awk \
#  dummy
#

#
# Iniziamo il BEGIN:
#
BEGIN {
    if (debug) print "# Starting update" > "/dev/stderr"

    read_weights(weights_file)
    read_inputs(input_file)
    read_gradients(grad_file)
    update_weights()
    write_weights(output_file)

    if (debug) print "# Done" > "/dev/stderr"
    exit
}

#
# Effettivo aggiornamento dei pesi, con debug
#
function update_weights(   i, j, k, grad, val, update, delta, old_w) {
    for (j = 1; j <= nrows; j++) {
        for (i = 1; i <= ncols; i++) {
            update = 0
            for (k = 1; k <= grad_rows; k++) {
                grad = D[k, j]
                val = X[k, i]
                update += grad * val
            }
            update /= grad_rows
            delta = learning_rate * update
            old_w = W[j, i]
            W[j, i] = old_w - delta
            if (debug) {
                printf "# neuron=%d i=%d old=%.6f update=%.6f new=%.6f\n", j, i, old_w, delta, W[j,i] > "/dev/stderr"
            }
        }
    }
}

